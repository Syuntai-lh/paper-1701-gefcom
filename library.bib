
@ARTICLE{Hong2019-ew,
  title   = "Global Energy Forecasting Competition 2017: Hierarchical
             Probabilistic Load Forecasting",
  author  = "Hong, Tao and Xie, Jingrui and Black, Jonathan",
  journal = "International journal of forecasting",
  year    =  2019
}

@ARTICLE{Ben_Taieb2017-it,
  title     = "Hierarchical Probabilistic Forecasting of Electricity Demand
               with Smart Meter Data",
  author    = "Ben Taieb, Souhaib and Taylor, James W and Hyndman, Rob J",
  abstract  = "Abstract Electricity smart meters record consumption, on a near
               real-time basis, at the level of individual commercial and
               residential properties. From this, a hierarchy can be
               constructed consisting of time series of demand at the smart
               meter level, and at various",
  publisher = "robjhyndman.com",
  year      =  2017,
  url       = "https://robjhyndman.com/papers/HPFelectricity.pdf"
}

@TECHREPORT{Ben_Taieb2017-yf,
  title       = "Coherent Probabilistic Forecasts for Hierarchical Time Series",
  author      = "Ben Taieb, Souhaib and Taylor, James W and Hyndman, Rob J and
                 {Others}",
  abstract    = "Abstract Many applications require forecasts for a hierarchy
                 comprising a set of time series along with aggregates of
                 subsets of these series. Although forecasts can be produced
                 independently for each series in the hierarchy, typically this
                 does not lead to coherent",
  publisher   = "business.monash.edu",
  number      = "Working Paper 03/17",
  institution = "Monash University, Department of Econometrics and Business
                 Statistics",
  year        =  2017,
  url         = "http://business.monash.edu/econometrics-and-business-statistics/research/publications/ebs/wp03-17.pdf"
}

@ARTICLE{Schapire1990-yl,
  title     = "The Strength of Weak Learnability",
  author    = "Schapire, Robert E",
  abstract  = "This paper addresses the problem of improving the accuracy of an
               hypothesis output by a learning algorithm in the
               distribution-free (PAC) learning model. A concept class is
               learnable (or strongly learnable) if, given access to a source
               of examples of the unknown concept, the learner with high
               probability is able to output an hypothesis that is correct on
               all but an arbitrarily small fraction of the instances. The
               concept class is weakly learnable if the learner can produce an
               hypothesis that performs only slightly better than random
               guessing. In this paper, it is shown that these two notions of
               learnability are equivalent.A method is described for converting
               a weak learning algorithm into one that achieves arbitrarily
               high accuracy. This construction may have practical applications
               as a tool for efficiently converting a mediocre learning
               algorithm into one that performs extremely well. In addition,
               the construction has some interesting theoretical consequences,
               including a set of general upper bounds on the complexity of any
               strong learning algorithm as a function of the allowed error
               $\in$.",
  journal   = "Machine learning",
  publisher = "Kluwer Academic Publishers-Plenum Publishers",
  volume    =  5,
  number    =  2,
  pages     = "197-227",
  month     =  jun,
  year      =  1990,
  url       = "https://link.springer.com/article/10.1023/A:1022648800760",
  language  = "en"
}

@ARTICLE{Friedman2000-st,
  title     = "Additive logistic regression: a statistical view of boosting
               (With discussion and a rejoinder by the authors)",
  author    = "Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert",
  abstract  = "Project Euclid - mathematics and statistics online",
  journal   = "Annals of statistics",
  publisher = "Institute of Mathematical Statistics",
  volume    =  28,
  number    =  2,
  pages     = "337-407",
  month     =  apr,
  year      =  2000,
  url       = "http://projecteuclid.org/euclid.aos/1016218223",
  keywords  = "classification; tree; nonparametric estimation; stagewise
               fitting; machine learning",
  language  = "en"
}

@ARTICLE{Xie2016-ru,
  title    = "Temperature Scenario Generation for Probabilistic Load
              Forecasting",
  author   = "Xie, Jingrui and Hong, Tao",
  abstract = "In today's dynamic and competitive business environment,
              probabilistic load forecasting (PLF) is becoming increasingly
              important to utilities for quantifying the uncertainties in the
              future. Among the various approaches to generating probabilistic
              load forecasts, feeding simulated weather scenarios to a point
              load forecasting model is being commonly accepted by the industry
              for its simplicity and interpretability. There are three
              practical and widely used methods for temperature scenario
              generation, namely fixed-date, shifted-date, and bootstrap
              methods. Nevertheless, these methods have been used mainly on an
              ad hoc basis without being formally compared or quantitatively
              evaluated. For instance, it has never been clear to the industry
              how many years of weather history is sufficient to adopt these
              methods. This is the first study to evaluate these three
              temperature scenario generation methods based on the quantile
              score, a comprehensive quantitative error measure for
              probabilistic forecasts. Through a series of empirical studies on
              both linear and nonlinear models with three different levels of
              predictive power, we find that 1) the quantile score of each
              method shows diminishing improvement as the length of available
              temperature history increases; 2) while shifting dates can
              compensate short weather history, the quantile score improvement
              gained from the shifted-date method diminishes and eventually
              becomes negative as the number of shifted days increases; and 3)
              comparing with the fixed-date method, the bootstrap method offers
              the capability of generating more comprehensive scenarios but
              does not improve the quantile score. At the end, an empirical
              formula for selecting and applying the temperature scenario
              generation methods is proposed together with a practical
              guideline.",
  journal  = "IEEE transactions on smart grid",
  volume   = "PP",
  number   =  99,
  pages    = "1-1",
  year     =  2016,
  url      = "http://dx.doi.org/10.1109/TSG.2016.2597178",
  keywords = "Forecasting;Load forecasting;Load modeling;Meteorology;Predictive
              models;Probabilistic logic;Temperature distribution;Electric load
              forecasting;neural networks;pinball loss function;probabilistic
              load forecasting;quantile score;regression models;scenario
              generation"
}

@ARTICLE{Hong2016-lo,
  title    = "Probabilistic energy forecasting: Global Energy Forecasting
              Competition 2014 and beyond",
  author   = "Hong, Tao and Pinson, Pierre and Fan, Shu and Zareipour,
              Hamidreza and Troccoli, Alberto and Hyndman, Rob J",
  abstract = "The energy industry has been going through a significant
              modernization process over the last decade. Its infrastructure is
              being upgraded rapidly. The supply, demand and prices are
              becoming more volatile and less predictable than ever before.
              Even its business model is being challenged fundamentally. In
              this competitive and dynamic environment, many decision-making
              processes rely on probabilistic forecasts to quantify the
              uncertain future. Although most of the papers in the energy
              forecasting literature focus on point or single-valued forecasts,
              the research interest in probabilistic energy forecasting
              research has taken off rapidly in recent years. In this paper, we
              summarize the recent research progress on probabilistic energy
              forecasting. A major portion of the paper is devoted to
              introducing the Global Energy Forecasting Competition 2014
              (GEFCom2014), a probabilistic energy forecasting competition with
              four tracks on load, price, wind and solar forecasting, which
              attracted 581 participants from 61 countries. We conclude the
              paper with 12 predictions for the next decade of energy
              forecasting.",
  journal  = "International Journal of Forecasting",
  volume   =  32,
  number   =  3,
  pages    = "896-913",
  year     =  2016,
  url      = "http://dx.doi.org/10.1016/j.ijforecast.2016.02.001",
  keywords = "Electric load forecasting; Electricity price forecasting;
              Forecasting competition; Probabilistic forecasting; Solar power
              forecasting; Wind power forecasting"
}

@ARTICLE{Ziel2016-yz,
  title    = "Lasso estimation for {GEFCom2014} probabilistic electric load
              forecasting",
  author   = "Ziel, Florian and Liu, Bidong",
  abstract = "We present a methodology for probabilistic load forecasting that
              is based on lasso (least absolute shrinkage and selection
              operator) estimation. The model considered can be regarded as a
              bivariate time-varying threshold autoregressive(AR) process for
              the hourly electric load and temperature. The joint modeling
              approach incorporates the temperature effects directly, and
              reflects daily, weekly, and annual seasonal patterns and public
              holiday effects. We provide two empirical studies, one based on
              the probabilistic load forecasting track of the Global Energy
              Forecasting Competition 2014 (GEFCom2014-L), and the other based
              on another recent probabilistic load forecasting competition that
              follows a setup similar to that of GEFCom2014-L. In both
              empirical case studies, the proposed methodology outperforms two
              multiple linear regression based benchmarks from among the top
              eight entries to GEFCom2014-L.",
  journal  = "International journal of forecasting",
  volume   =  32,
  number   =  3,
  pages    = "1029-1037",
  year     =  2016,
  url      = "http://dx.doi.org/10.1016/j.ijforecast.2016.01.001"
}

@ARTICLE{Gneiting2011-wu,
  title    = "Quantiles as optimal point forecasts",
  author   = "Gneiting, Tilmann",
  abstract = "Loss functions play a central role in the theory and practice of
              forecasting. If the loss function is quadratic, the mean of the
              predictive distribution is the unique optimal point predictor. If
              the loss is symmetric piecewise linear, any median is an optimal
              point forecast. Quantiles arise as optimal point forecasts under
              a general class of economically relevant loss functions, which
              nests the asymmetric piecewise linear loss, and which we refer to
              as generalized piecewise linear (GPL). The level of the quantile
              depends on a generic asymmetry parameter which reflects the
              possibly distinct costs of underprediction and overprediction.
              Conversely, a loss function for which quantiles are optimal point
              forecasts is necessarily GPL. We review characterizations of this
              type in the work of Thomson, Saerens and Komunjer, and relate to
              proper scoring rules, incentive-compatible compensation schemes
              and quantile regression. In the empirical part of the paper, the
              relevance of decision theoretic guidance in the transition from a
              predictive distribution to a point forecast is illustrated using
              the Bank of England's density forecasts of United Kingdom
              inflation rates, and probabilistic predictions of wind energy
              resources in the Pacific Northwest. ?? 2010 International
              Institute of Forecasters.",
  journal  = "International journal of forecasting",
  volume   =  27,
  number   =  2,
  pages    = "197-207",
  year     =  2011,
  url      = "http://dx.doi.org/10.1016/j.ijforecast.2009.12.015",
  keywords = "Decision making; Density forecasts; Incentive-compatible
              compensation scheme; Loss function; Piecewise linear; Proper
              scoring rule; Quantile"
}

@ARTICLE{Ben_Taieb2014-rr,
  title    = "A gradient boosting approach to the Kaggle load forecasting
              competition",
  author   = "Ben Taieb, Souhaib and Hyndman, Rob J",
  abstract = "We describe and analyse the approach used by Team TinTin (Souhaib
              Ben Taieb and Rob J Hyndman) in the Load Forecasting track of the
              Kaggle Global Energy Forecasting Competition 2012. The
              competition involved a hierarchical load forecasting problem for
              a US utility with 20 geographical zones. The data available
              consisted of the hourly loads for the 20 zones and hourly
              temperatures from 11 weather stations, for four and a half years.
              For each zone, the hourly electricity loads for nine different
              weeks needed to be predicted without having the locations of
              either the zones or stations. We used separate models for each
              hourly period, with component-wise gradient boosting for
              estimating each model using univariate penalised regression
              splines as base learners. The models allow for the electricity
              demand changing with the time-of-year, day-of-week, time-of-day,
              and on public holidays, with the main predictors being current
              and past temperatures, and past demand. Team TinTin ranked fifth
              out of 105 participating teams.",
  journal  = "International Journal of Forecasting",
  volume   =  30,
  number   =  2,
  pages    = "382-394",
  month    =  apr,
  year     =  2014,
  url      = "http://linkinghub.elsevier.com/retrieve/pii/S0169207013000812"
}

@ARTICLE{Hyndman2010-ui,
  title    = "Density forecasting for long-term peak electricity demand",
  author   = "Hyndman, Rob J and Fan, Shu",
  abstract = "Long-term electricity demand forecasting plays an important role
              in planning for future generation facilities and transmission
              augmentation. In a long-term context, planners must adopt a
              probabilistic view of potential peak demand levels. Therefore
              density forecasts (providing estimates of the full probability
              distributions of the possible future values of the demand) are
              more helpful than point forecasts, and are necessary for
              utilities to evaluate and hedge the financial risk accrued by
              demand variability and forecasting uncertainty. This paper
              proposes a new methodology to forecast the density of long-term
              peak electricity demand. Peak electricity demand in a given
              season is subject to a range of uncertainties, including
              underlying population growth, changing technology, economic
              conditions, prevailing weather conditions (and the timing of
              those conditions), as well as the general randomness inherent in
              individual usage. It is also subject to some known calendar
              effects due to the time of day, day of week, time of year, and
              public holidays. A comprehensive forecasting solution is
              described in this paper. First, semi-parametric additive models
              are used to estimate the relationships between demand and the
              driver variables, including temperatures, calendar effects and
              some demographic and economic variables. Then the demand
              distributions are forecasted by using a mixture of temperature
              simulation, assumed future economic scenarios, and residual
              bootstrapping. The temperature simulation is implemented through
              a new seasonal bootstrapping method with variable blocks. The
              proposed methodology has been used to forecast the probability
              distribution of annual and weekly peak electricity demand for
              South Australia since 2007. The performance of the methodology is
              evaluated by comparing the forecast results with the actual
              demand of the summer 2007-2008.",
  journal  = "IEEE Transactions on Power Systems",
  volume   =  25,
  number   =  2,
  pages    = "1142-1153",
  month    =  may,
  year     =  2010,
  url      = "http://ieeexplore.ieee.org/document/5345698/",
  keywords = "Density forecast; Long-term demand forecasting; Simulation; Time
              series"
}

@ARTICLE{Hyndman2011-kg,
  title    = "Optimal combination forecasts for hierarchical time series",
  author   = "Hyndman, Rob J and Ahmed, Roman A and Athanasopoulos, George and
              Shang, Han Lin",
  abstract = "In many applications, there are multiple time series that are
              hierarchically organized and can be aggregated at several
              different levels in groups based on products, geography or some
              other features. We call these ``hierarchical time series''. They
              are commonly forecast using either a ``bottom-up'' or a
              ``top-down'' method. In this paper we propose a new approach to
              hierarchical forecasting which provides optimal forecasts that
              are better than forecasts produced by either a top-down or a
              bottom-up approach. Our method is based on independently
              forecasting all series at all levels of the hierarchy and then
              using a regression model to optimally combine and reconcile these
              forecasts. The resulting revised forecasts add up appropriately
              across the hierarchy, are unbiased and have minimum variance
              amongst all combination forecasts under some simple assumptions.
              We show in a simulation study that our method performs well
              compared to the top-down approach and the bottom-up method. We
              demonstrate our proposed method by forecasting Australian tourism
              demand where the data are disaggregated by purpose of travel and
              geographical region.",
  journal  = "Computational statistics \& data analysis",
  volume   =  55,
  number   =  9,
  pages    = "2579-2589",
  month    =  sep,
  year     =  2011,
  url      = "http://linkinghub.elsevier.com/retrieve/pii/S0167947311000971"
}

@ARTICLE{Hyndman2016-ad,
  title    = "Fast computation of reconciled forecasts for hierarchical and
              grouped time series",
  author   = "Hyndman, Rob J and Lee, Alan J and Wang, Earo",
  abstract = "It is shown that the least squares approach to reconciling
              hierarchical time series forecasts can be extended to much more
              general collections of time series with aggregation constraints.
              The constraints arise due to the need for forecasts of
              collections of time series to add up in the same way as the
              observed time series. It is also shown that the computations
              involved can be handled efficiently by exploiting the structure
              of the associated design matrix, or by using sparse matrix
              routines. The proposed algorithms make forecast reconciliation
              feasible in business applications involving very large numbers of
              time series.",
  journal  = "Computational statistics \& data analysis",
  volume   =  97,
  pages    = "16-32",
  year     =  2016,
  url      = "http://dx.doi.org/10.1016/j.csda.2015.11.007"
}

@ARTICLE{Wickramasuriya2015-lj,
  title    = "Forecasting hierarchical and grouped time series through trace
              minimization",
  author   = "Wickramasuriya, Shanika L and Athanasopoulos, George and Hyndman,
              Rob J and {Others}",
  abstract = "Large collections of time series often have aggregation
              constraints due to product or geographical hierarchies. The
              forecasts for the disaggregated series are usually required to
              add up exactly to the forecasts of the aggregated series, a
              constraint known as `` aggregate consistency '' . The combination
              forecasts proposed by Hyndman et al. (2011) are based on a
              Generalized Least Squares (GLS) estimator and require an estimate
              of the covariance matrix of the reconciliation errors (i.e., the
              errors that arise due to aggregate inconsistency). We show that
              this is impossible to estimate in practice due to identifiability
              conditions. We propose a new combination forecasting approach
              that incorporates the information from a full covariance matrix
              of forecast errors in obtaining a set of aggregate consistent
              forecasts. Our approach minimizes the mean squared error of the
              aggregate consistent forecasts across the en-tire collection of
              time series under the assumption of unbiasedness. The
              minimization problem has a closed form solution. We make this
              solution scalable by providing a computationally less demanding
              alternative representation. We evaluate the performance of the
              proposed method compared to alternative methods using a series of
              simulation designs which take into account various features of
              the collected time series. This is followed by an empirical
              application using Australian domestic tourism data. The results
              indicate that the proposed method works well with artificial and
              real data.",
  journal  = "Department of Econometrics and Business Statistics, Monash
              University",
  number   = "Working Paper 15/15",
  year     =  2015,
  url      = "http://business.monash.edu/old/econometrics-and-business-statistics/research/publications/ebs/wp15-15.pdf",
  keywords = "Hierarchical time series; contemporaneous error correla-tion;
              forecasting; reconciliation; trace minimization"
}

@INPROCEEDINGS{Chen2016-bg,
  title     = "{XGBoost}: A Scalable Tree Boosting System",
  booktitle = "Proceedings of the 22nd {ACM} {SIGKDD} International Conference
               on Knowledge Discovery and Data Mining",
  author    = "Chen, Tianqi and Guestrin, Carlos",
  abstract  = "Tree boosting is a highly effective and widely used machine
               learning method. In this paper, we describe a scalable end-
               to-end tree boosting system called XGBoost, which is used widely
               by data scientists to achieve state-of-the-art results on many
               machine learning challenges. We propose a novel sparsity-aware
               algorithm for sparse data and weighted quan- tile sketch for
               approximate tree learning. More importantly, we provide insights
               on cache access patterns, data compres- sion and sharding to
               build a scalable tree boosting system. By combining these
               insights, XGBoost scales beyond billions of examples using far
               fewer resources than existing systems.",
  publisher = "ACM",
  pages     = "785-794",
  month     =  aug,
  year      =  2016,
  address   = "New York, New York, USA",
  keywords  = "large-scale machine learning"
}

@ARTICLE{Friedman2001-bz,
  title     = "Greedy Function Approximation: A Gradient Boosting Machine",
  author    = "Friedman, Jerome",
  abstract  = "Function estimation/approximation is viewed from the perspective
               of numerical optimization in function space, rather than
               parameter space. A connection is made between stagewise additive
               expansions and steepest-descent minimization. A general gradient
               descent ``boosting'' paradigm is developed for additive
               expansions based on any fitting criterion. Specific algorithms
               are presented for least-squares, least absolute deviation, and
               Huber-M loss functions for regression, and multiclass logistic
               likelihood for classification. Special enhancements are derived
               for the particular case where the individual additive components
               are regression trees, and tools for interpreting such
               ``TreeBoost'' models are presented. Gradient boosting of
               regression trees produces competitive, highly robust,
               interpretable procedures for both regression and classification,
               especially appropriate for mining less than clean data.
               Connections between this approach and the boosting methods of
               Freund and Shapire and Friedman, Hastie and Tibshirani are
               discussed.",
  journal   = "Annals of statistics",
  publisher = "Institute of Mathematical Statistics",
  volume    =  29,
  number    =  5,
  pages     = "1189-1232",
  year      =  2001,
  url       = "http://www.jstor.org/stable/2699986"
}

@ARTICLE{Koren2009-pd,
  title    = "The {BellKor} solution to the Netflix grand prize",
  author   = "Koren, Yehuda",
  abstract = "This article describes part of our contribution to the ``Bell -
              Kor's Pragmatic Chaos'' final solution, which won the Netflix
              Grand Prize. The other portion of the contribution was creat ed
              while working at AT\&T with Robert Bell and Chris Volinsky, as
              reported in our 2008 Progress Prize report [3]. The final
              solution includes all the predictors described there. In th is
              article we describe only the newer predictors. So what is new
              over last year's solution? First we further im- proved the
              baseline predictors (Sec. III). This in turn impr oves our other
              models, which incorporate those predictors, like the matrix
              factorization model (Sec. IV). In addition, an exten sion of the
              neighborhood model that addresses temporal dynamics was
              introduced (Sec. V). On the Restricted Boltzmann Ma- chines (RBM)
              front, we use a new RBM model with superior accuracy by
              conditioning the visible units (Sec. VI). The fin al addition is
              the introduction of a new blending algorithm, wh ich is based on
              gradient boosted decision trees (GBDT) (Sec. VII ).",
  journal  = "Netflix prize documentation",
  volume   =  81,
  number   = "August",
  pages    = "1-10",
  year     =  2009,
  url      = "http://www.stat.osu.edu/~dmsl/GrandPrize2009_BPC_BellKor.pdf"
}

@ARTICLE{Hong2014-bb,
  title    = "Global energy forecasting competition 2012",
  author   = "Hong, Tao and Pinson, Pierre and Fan, Shu",
  abstract = "The Global Energy Forecasting Competition (GEFCom2012) attracted
              hundreds of participants worldwide, who contributed many novel
              ideas to the energy forecasting field. This paper introduces both
              tracks of GEFCom2012, hierarchical load forecasting and wind
              power forecasting, with details on the aspects of the problem,
              the data, and a summary of the methods used by selected top
              entries. We also discuss the lessons learned from this
              competition from the organizers' perspective. The complete data
              set, including the solution data, is published along with this
              paper, in an effort to establish a benchmark data pool for the
              community. ?? 2013 International Institute of Forecasters.",
  journal  = "International Journal of Forecasting",
  volume   =  30,
  number   =  2,
  pages    = "357-363",
  year     =  2014,
  url      = "http://dx.doi.org/10.1016/j.ijforecast.2013.07.001"
}

@PHDTHESIS{Hong2010-cy,
  title  = "Short term electric load forecasting",
  author = "Hong, Tao",
  year   =  2010,
  school = "North Carolina State University"
}
